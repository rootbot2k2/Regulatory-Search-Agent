
# Regulatory Search Agent: System Architecture

**Author:** Manus AI
**Date:** November 4, 2025

## 1. Introduction

This document outlines the comprehensive system architecture for the **Regulatory Search Agent**, an advanced AI system designed to automate the retrieval, processing, and analysis of drug regulatory documents from major international health agencies. The agent will provide a powerful, user-friendly interface for deep research and synthesis, leveraging a Retrieval-Augmented Generation (RAG) model to answer complex queries based on a continuously updated knowledge base of regulatory information.

### 1.1. Project Goal

The primary objective is to create an autonomous agent capable of navigating the websites of six key regulatory bodies (EMA, FDA, Health Canada, TGA, NHRA, Swissmedic), locating and downloading relevant drug approval documents, indexing them into a local vector store (FAISS), and providing a chat-based interface for users to query this specialized knowledge base. The system will be built with a modular architecture, allowing for future expansion and integration.

### 1.2. Core Capabilities

- **Autonomous Web Browsing:** The agent will be able to navigate complex web pages, fill out forms, and interact with search functionalities on regulatory websites.
- **On-Demand Document Retrieval:** Documents will be fetched based on user queries for specific drugs or topics, rather than bulk downloading all available data.
- **Local Vector Indexing:** Retrieved documents will be processed, chunked, and indexed locally using FAISS for efficient similarity search.
- **RAG-Powered Q&A:** A user-friendly chat interface will allow users to ask questions and receive answers generated by an LLM (GPT-4) grounded in the retrieved regulatory documents.
- **Multi-Agency Support:** The system will be designed to handle the unique website structures and document formats of all six specified regulatory agencies.

## 2. System Architecture

The Regulatory Search Agent is designed with a modular, service-oriented architecture. This separation of concerns ensures maintainability, scalability, and the ability to independently upgrade components. The system is composed of five core modules:

```mermaid
graph TD
    A[User Interface (GUI)] --> B{Agentic Core / Orchestrator};
    B --> C[Web Automation Module];
    C --> D[Document Processing & Indexing Module];
    D --> E[Vector Store (FAISS)];
    B --> F[RAG & Q&A Module];
    F --> E;
    F --> G[OpenAI API (GPT-4)];
    A --> F;

    subgraph User-Facing
        A
    end

    subgraph Backend Services
        B
        C
        D
        F
    end

    subgraph Data Layer
        E
    end

    subgraph External Services
        G
    end
```

### 2.1. Module Descriptions

| Module | Description | Key Responsibilities |
| :--- | :--- | :--- |
| **User Interface (GUI)** | A simple, web-based chat interface for user interaction. | - Submitting user queries (drug names, topics).<br>- Displaying generated answers and source documents.<br>- Allowing selection of different OpenAI models. |
| **Agentic Core / Orchestrator** | The central "brain" of the system that interprets user queries and coordinates the other modules. | - Receiving user requests from the GUI.<br>- Devising a plan to fulfill the request (e.g., which agency to search).<br>- Invoking the Web Automation Module to find and download documents.<br>- Triggering the Document Processing & Indexing Module.<br>- Passing the query to the RAG & Q&A Module. |
| **Web Automation Module** | A dedicated service for interacting with regulatory agency websites. | - Navigating to target agency websites.<br>- Performing searches for specific drugs or documents.<br>- Downloading PDF or other document files.<br>- Handling website-specific logic (e.g., parsing different HTML structures). |
| **Document Processing & Indexing Module** | Responsible for preparing and indexing the content of retrieved documents. | - Extracting text from downloaded PDF files.<br>- Chunking the text into manageable segments.<br>- Generating vector embeddings for each chunk using an OpenAI model.<br>- Storing the embeddings and associated text in a FAISS index. |
| **RAG & Q&A Module** | The core component for answering user questions. | - Receiving a user query and generating a query embedding.<br>- Performing a similarity search against the FAISS index to find relevant document chunks.<br>- Constructing a prompt for the LLM (GPT-4) that includes the user's question and the retrieved context.<br>- Returning the LLM-generated answer to the Agentic Core. |


## 3. Technology Stack

To ensure a robust and modern application, the following technology stack is recommended. This stack leverages a combination of Python for the backend and AI-related tasks, and a simple, modern frontend framework for the user interface.

| Component | Technology | Rationale |
| :--- | :--- | :--- |
| **Backend Framework** | FastAPI (Python) | High-performance, asynchronous, and well-suited for building APIs. Its dependency injection system simplifies code organization. |
| **Frontend Framework** | Streamlit or Gradio (Python) | For a simple GUI, these Python-based frameworks are the fastest way to build a chat interface without needing a separate JavaScript frontend. This simplifies development and maintenance. |
| **Web Automation** | Selenium with WebDriver | A powerful and flexible library for browser automation, capable of handling complex interactions with modern websites. |
| **PDF Text Extraction** | PyMuPDF (fitz) | A high-performance Python library for PDF processing, known for its speed and accuracy in text extraction. |
| **Vector Database** | FAISS (Facebook AI Similarity Search) | A library for efficient similarity search and clustering of dense vectors. It's lightweight, runs locally, and is ideal for prototyping and single-machine deployments. |
| **LLM & Embeddings** | OpenAI API (GPT-4, text-embedding-ada-002) | Provides state-of-the-art language models for generation and high-quality embeddings for retrieval. |
| **Environment Management** | Poetry or Pipenv | For managing Python dependencies and creating a reproducible environment. |

## 4. Detailed Module Descriptions

### 4.1. User Interface (GUI)

The GUI will be a simple, single-page web application with a chat interface. It will be built using a Python-based framework like Streamlit or Gradio to minimize development complexity.

**Key Features:**
- A text input box for users to enter their queries.
- A display area for the agent's responses, including the generated text and citations of the source documents.
- A dropdown menu to select the OpenAI model to be used for the RAG response (e.g., GPT-4, GPT-3.5-turbo).
- A status indicator to show when the agent is processing a request.

### 4.2. Agentic Core / Orchestrator

This module is the heart of the system, responsible for managing the overall workflow. It will be implemented as a set of services within the FastAPI backend.

**Workflow:**
1.  Receive a user query from the GUI.
2.  Analyze the query to identify the drug name, topic, and any other relevant entities.
3.  Based on the query, determine a strategy for which regulatory agencies to search. This could be a simple, predefined list or a more dynamic approach.
4.  For each target agency, invoke the Web Automation Module to find and download the relevant documents.
5.  Once the documents are downloaded, trigger the Document Processing & Indexing Module to process and index them.
6.  After indexing is complete, pass the user's original query to the RAG & Q&A Module.
7.  Receive the generated answer and source information from the RAG module.
8.  Format the response and send it back to the GUI for display.

### 4.3. Web Automation Module

This module will contain the logic for interacting with each of the six regulatory agency websites. It will be designed to be highly modular, with a separate sub-module for each agency.

**Agency-Specific Logic:**
- **FDA:** Utilize the downloadable data files to get direct URLs to documents, falling back to web scraping if necessary.
- **EMA:** Parse the nightly Excel files to get metadata and links to medicine pages.
- **Health Canada:** Interact with the DHPP search portal to find and download RDS documents.
- **TGA:** Use the ARTG search tool to locate and download PI and CMI documents.
- **Swissmedic:** Navigate the AIPS portal to find and retrieve product information.
- **NHRA:** Focus on downloading available PDF guidelines, as a searchable public database is not readily available.

### 4.4. Document Processing & Indexing Module

This module is responsible for the ETL (Extract, Transform, Load) pipeline for the regulatory documents.

**Pipeline:**
1.  **Extraction:** Use PyMuPDF to extract raw text from downloaded PDF documents.
2.  **Transformation:**
    -   Clean the extracted text to remove headers, footers, and other artifacts.
    -   Chunk the text into smaller, overlapping segments (e.g., 500-1000 characters with a 100-character overlap). This is crucial for providing relevant context to the LLM.
3.  **Loading & Indexing:**
    -   For each text chunk, call the OpenAI API to generate a vector embedding.
    -   Store the chunk and its corresponding embedding in a FAISS index. A separate metadata store will be used to link the chunks back to their source document.

### 4.5. RAG & Q&A Module

This module will handle the question-answering part of the workflow.

**Process:**
1.  Receive the user's query from the Agentic Core.
2.  Generate a query embedding for the user's question using the same OpenAI model used for document chunking.
3.  Use the FAISS index to perform a similarity search and retrieve the top-k most relevant document chunks.
4.  Construct a detailed prompt for the GPT-4 model. This prompt will include:
    -   A system message defining the agent's persona (e.g., "You are a helpful regulatory affairs assistant...").
    -   The retrieved document chunks as context.
    -   The user's original question.
5.  Send the prompt to the OpenAI API and receive the generated answer.
6.  Post-process the answer to add citations and references to the source documents.
7.  Return the final answer and source information to the Agentic Core.

## 5. Implementation Plan

This section provides a detailed, milestone-based plan for developing the Regulatory Search Agent. This phased approach ensures that the project is built in a structured and manageable way, with clear deliverables at each stage.

### 5.1. Milestone 1: Project Setup and Core Backend

**Goal:** Establish the foundational backend infrastructure, including the FastAPI application, project structure, and environment configuration.

| Task | Description |
| :--- | :--- |
| **1.1. Initialize Project** | Create a new Git repository named `Regulatory-Search-Agent`. Set up a Python environment using Poetry or Pipenv. |
| **1.2. Set up FastAPI** | Install FastAPI and Uvicorn. Create a main application file (`main.py`) with a basic "Hello World" endpoint. |
| **1.3. Create Project Structure** | Organize the project into a logical directory structure (see section 6 for the recommended structure). |
| **1.4. Environment Configuration** | Create a `.env.template` file with a placeholder for the `OPENAI_API_KEY`. Load environment variables in the application. |
| **1.5. Health Check Endpoint** | Implement a `/health` endpoint that returns a 200 OK status to verify that the backend is running. |

### 5.2. Milestone 2: Document Processing and Indexing

**Goal:** Develop the services responsible for processing downloaded documents and indexing them into the FAISS vector store.

| Task | Description |
| :--- | :--- |
| **2.1. PDF Parsing Service** | Create a `PDFParserService` that takes a PDF file path and returns the extracted text content using PyMuPDF. |
| **2.2. Text Chunking Service** | Implement a `TextChunkingService` that takes a long text and splits it into smaller, overlapping chunks. |
| **2.3. FAISS Vector Store Service** | Create a `VectorStoreService` that encapsulates all interactions with FAISS, including methods for adding vectors and searching for similar vectors. |
| **2.4. Document Indexing Pipeline** | Combine the above services into a single pipeline that takes a document, processes it, and indexes it into FAISS. |
| **2.5. Testing Script** | Write a standalone script to test the indexing pipeline with a sample PDF document. |

### 5.3. Milestone 3: Web Automation Module

**Goal:** Build the web automation components to retrieve regulatory documents from the target agency websites.

| Task | Description |
| :--- | :--- |
| **3.1. Selenium Setup** | Configure Selenium with a WebDriver (e.g., ChromeDriver) to be used for browser automation. |
| **3.2. Agency Scraper (FDA)** | Implement a scraper for the FDA website as the first proof-of-concept. This scraper should be able to search for a drug and download the review documents. |
| **3.3. Generic Scraper Interface** | Define a generic interface for all agency scrapers to ensure a consistent API for the Agentic Core. |
| **3.4. Integration with Indexing** | Connect the Web Automation Module to the Document Processing & Indexing Module so that downloaded documents are automatically indexed. |
| **3.5. Expand to Other Agencies** | Sequentially implement scrapers for the other five agencies, handling their unique website structures. |

### 5.4. Milestone 4: RAG and Q&A Module

**Goal:** Implement the core RAG functionality to answer user questions based on the indexed documents.

| Task | Description |
| :--- | :--- |
| **4.1. OpenAI Integration** | Create a service to interact with the OpenAI API for generating embeddings and chat completions. |
| **4.2. RAG Service** | Implement the `RAGService` that takes a user query, retrieves relevant context from the FAISS vector store, and generates an answer using the LLM. |
| **4.3. Prompt Engineering** | Develop and refine the system prompt to guide the LLM in generating accurate and well-cited answers. |
| **4.4. API Endpoint for Q&A** | Create a FastAPI endpoint that exposes the RAG service, allowing for testing of the question-answering functionality. |

### 5.5. Milestone 5: GUI and Final Integration

**Goal:** Build the user-facing chat interface and integrate all the backend modules into a cohesive application.

| Task | Description |
| :--- | :--- |
| **5.1. Build GUI** | Use Streamlit or Gradio to create a simple chat interface. |
| **5.2. Backend Integration** | Connect the GUI to the FastAPI backend, allowing users to submit queries and see the results. |
| **5.3. End-to-End Workflow** | Implement the full end-to-end workflow, from user query to document retrieval, indexing, and RAG-based response. |
| **5.4. Model Selection** | Add the dropdown menu to the GUI to allow users to select the OpenAI model for the RAG response. |
| **5.5. User Feedback Mechanism** | (Optional) Add a simple feedback mechanism (e.g., thumbs up/down) to collect user feedback on the quality of the answers. |

## 6. Project Structure

A well-organized project structure is essential for maintainability. The following directory structure is recommended:

```
/Regulatory-Search-Agent
|-- .env
|-- .env.template
|-- .gitignore
|-- README.md
|-- main.py
|-- requirements.txt
|-- app/
|   |-- __init__.py
|   |-- api/
|   |   |-- __init__.py
|   |   |-- endpoints.py
|   |-- core/
|   |   |-- __init__.py
|   |   |-- config.py
|   |   |-- orchestrator.py
|   |-- services/
|   |   |-- __init__.py
|   |   |-- document_processing.py
|   |   |-- rag_service.py
|   |   |-- vector_store.py
|   |   |-- web_automation/
|   |       |-- __init__.py
|   |       |-- base_scraper.py
|   |       |-- fda_scraper.py
|   |       |-- ema_scraper.py
|   |       |-- ...
|   |-- gui/
|       |-- __init__.py
|       |-- chat_interface.py
|-- data/
|   |-- downloaded_docs/
|   |-- faiss_index/
```

## 7. Best Practices

- **Modularity:** Keep each service focused on a single responsibility. This makes the code easier to test, debug, and maintain.
- **Error Handling:** Implement robust error handling, especially in the web automation and document processing modules. Use try-except blocks to catch and log errors gracefully.
- **Asynchronous Operations:** Use FastAPI's asynchronous capabilities to handle long-running tasks like web scraping and document processing without blocking the main application thread.
- **Configuration Management:** Store all configuration values (API keys, file paths, etc.) in environment variables or a dedicated configuration file. Never hard-code them in the source code.
- **Logging:** Implement comprehensive logging to track the agent's activities, including which documents are being downloaded, which queries are being processed, and any errors that occur.
- **Testing:** Write unit tests for individual services and integration tests for the complete workflow to ensure the system is working as expected.

## 8. Technical Considerations

### 8.1. FAISS Index Management

FAISS is a powerful library for similarity search, but it requires careful management, especially as the index grows. Here are some key considerations:

- **Index Type:** For a prototype, a simple `IndexFlatL2` or `IndexFlatIP` (inner product) is sufficient. For larger datasets, consider using an `IndexIVFFlat` or `IndexHNSW` for faster search times at the cost of some accuracy.
- **Persistence:** FAISS indices can be saved to disk using `faiss.write_index()` and loaded with `faiss.read_index()`. This allows the agent to persist the index between sessions.
- **Metadata Management:** FAISS only stores vectors, not the associated text or metadata. A separate data structure (e.g., a Python dictionary or a simple SQLite database) should be used to map vector IDs to their corresponding text chunks and source document information.
- **Incremental Indexing:** As new documents are downloaded and processed, they should be added to the existing FAISS index. This can be done by loading the index, adding the new vectors, and saving it again.

### 8.2. Web Scraping Challenges

Web scraping is inherently fragile, as websites can change their structure at any time. Here are some strategies to mitigate this:

- **Robust Selectors:** Use CSS selectors or XPath expressions that are as specific as possible, but avoid relying on overly specific paths that might break with minor website updates.
- **Error Handling:** Implement comprehensive error handling to catch and log failures during web scraping. The agent should be able to gracefully handle situations where a website is unavailable or a document cannot be found.
- **Rate Limiting:** Implement rate limiting to avoid overwhelming the target websites with requests. This is both a matter of being a good internet citizen and avoiding being blocked by the website.
- **User-Agent Spoofing:** Some websites block requests from automated tools. Setting a realistic user-agent string in the Selenium WebDriver can help avoid this.

### 8.3. OpenAI API Usage and Cost

The OpenAI API is a paid service, and costs can add up, especially when generating embeddings for large documents. Here are some strategies to manage costs:

- **Caching:** Cache the embeddings for document chunks to avoid regenerating them every time the agent is restarted.
- **Model Selection:** Use the most cost-effective model that meets the requirements. For embeddings, `text-embedding-ada-002` is a good balance of quality and cost. For generation, `gpt-3.5-turbo` is significantly cheaper than `gpt-4` and may be sufficient for many use cases.
- **Prompt Optimization:** Keep prompts concise to reduce the number of tokens consumed by each request.

### 8.4. Security Considerations

- **API Key Management:** The OpenAI API key should be stored in an environment variable and never committed to the Git repository. The `.env.template` file should provide a clear example of how to set this up.
- **Input Validation:** Validate all user inputs to prevent injection attacks or other malicious behavior.
- **Secure Communication:** If the application is deployed on a server, use HTTPS to encrypt communication between the client and the server.

## 9. Deployment

For a prototype, the application can be run locally on the developer's machine. However, for production use, the following deployment options are recommended:

- **Docker:** Containerize the application using Docker to ensure a consistent environment across different machines. A `Dockerfile` and `docker-compose.yml` file should be provided.
- **Cloud Hosting:** Deploy the application to a cloud provider like AWS, Google Cloud, or Azure. This allows for easy scaling and management.
- **Continuous Integration/Continuous Deployment (CI/CD):** Set up a CI/CD pipeline using tools like GitHub Actions or GitLab CI to automate the testing and deployment process.

## 10. Future Enhancements

The initial version of the Regulatory Search Agent will provide a solid foundation, but there are many opportunities for future enhancements:

- **Advanced Query Understanding:** Use natural language processing techniques to better understand user queries and extract entities like drug names, indications, and regulatory concerns.
- **Multi-Document Synthesis:** Extend the RAG module to synthesize information from multiple documents and provide a more comprehensive answer.
- **Comparative Analysis:** Allow users to compare regulatory decisions across different agencies for the same drug.
- **Automatic Document Updates:** Implement a background process that periodically checks for new documents on the regulatory agency websites and automatically downloads and indexes them.
- **User Authentication and Personalization:** Add user authentication to allow for personalized experiences, such as saving favorite queries or documents.
- **Advanced Vector Database:** Migrate from FAISS to a more scalable vector database like Pinecone, Weaviate, or Milvus for production deployments.
- **Support for More Agencies:** Expand the system to support additional regulatory agencies beyond the initial six.

## 11. Conclusion

The Regulatory Search Agent represents a significant step forward in automating the retrieval and analysis of drug regulatory information. By combining advanced web automation, state-of-the-art natural language processing, and a user-friendly interface, this system will empower researchers, regulatory professionals, and other stakeholders to access and understand regulatory data more efficiently.

This architecture document provides a comprehensive blueprint for building the system, from the initial setup to the final deployment. By following the outlined milestones and best practices, the development team can ensure that the project is delivered on time, within budget, and to the highest quality standards.

The modular design of the system ensures that it can be easily extended and adapted to meet future needs, making it a valuable long-term investment for any organization working in the regulatory space.
